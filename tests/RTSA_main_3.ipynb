{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***RTSA Version 3***: Optimization, TipLets & WristLets Integration, Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation\n",
    "\n",
    "- Visual Studio Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **A. Setup & Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Debug Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# debug = True\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if debug: \n",
    "    # choose test image\n",
    "    test_name = 'desk'\n",
    "    # test_name = 'desk' # options: desk, guy, hand, backpack\n",
    "    # test_name = 'hand' # options: desk, guy, hand, backpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import dearpygui.dearpygui as dpg\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import re\n",
    "import json\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# viz_res = [640, 480]\n",
    "viz_res = tuple([400, 300])\n",
    "# window_size = [1400, 800]\n",
    "window_size = [1400, 1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A0. Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdpg2(img):\n",
    "    return img.astype(np.float32)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_imdpg2(imgs):\n",
    "    return imgs.astype(np.float32)/255\n",
    "\n",
    "# to use it, add the following lines of code @ DPG main loop, \"Push Frames\" section:\n",
    "    # img_, edge_, seg_overlay_, masked_edge_, virtual_fov_, hand_tracking_, ofa_ = batch_imdpg2(np.stack((img, edge, seg_overlay, masked_edge[...,:3], virtual_fov, hand_tracking, ofa)))\n",
    "    # dpg.set_value(\"original\",      img_)\n",
    "    # dpg.set_value(\"edge\",          edge_) \n",
    "    # dpg.set_value(\"segmentation\",  seg_overlay_)\n",
    "    # dpg.set_value(\"masked_edge\",   masked_edge_)\n",
    "    # dpg.set_value(\"virtual_fov\",   virtual_fov_)\n",
    "    # dpg.set_value(\"hand_tracking\", hand_tracking_)\n",
    "    # # dpg.set_value(\"tiplets\",       tiplets_)\n",
    "    # dpg.set_value(\"ofa\",           ofa_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rgba(img, mask=None):\n",
    "    if mask is None: mask = np.zeros(img.shape[:2])+255 # white mask\n",
    "    rgba = cv2.cvtColor(img, cv2.COLOR_RGB2RGBA) # convert to rgba\n",
    "    rgba[:, :, 3] = np.interp(mask, (0, mask.max()), (0,255)) # rgba[:, :, 3] = mask # set alpha channel\n",
    "    return rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def blend(rgba): # blend w/ zero brightness background\n",
    "    alpha = np.interp(cv2.cvtColor(rgba[:,:,3], cv2.COLOR_GRAY2RGB), (0,255), (0,1))\n",
    "    return (rgba[:,:,:3]*alpha).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def blendtwo(rgba, rgb):\n",
    "    alpha = (rgba[:,:,3].astype(np.float32)/255)[:,:,None] # add dummy axis @ dim=3\n",
    "    return (rgba[:,:,:3]*alpha + rgb*(1-alpha)).astype(np.uint8)\n",
    "# Study Reference: https://note.nkmk.me/en/python-opencv-numpy-alpha-blend-mask/\n",
    "\n",
    "# due to notebook's order, run test case after spinning up Section A \n",
    "# plt.imshow(blendtwo(rgba(edge, seg_mask), img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blendtwo2(rgba, rgb):\n",
    "    alpha = (rgba[...,3].astype(np.float32)/255)[...,None] # add dummy axis @ dim=3\n",
    "    return (rgba[...,:3]*alpha + rgb*(1-alpha)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addWeighted seems to only support **scalar** alpha values. let's fall back to blendtwo2.\n",
    "def blendtwo3(rgba, rgb):\n",
    "    alpha = rgba[...,3].astype(np.float32)/255; i_alpha = 1 - alpha\n",
    "    return cv2.addWeighted(rgb[...,:3], alpha, rgba, i_alpha, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A1. Input Camera Stream**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects a webcam connected to the PC. If multiple webcams are present, try `cv2.VideoCapture(1, 2, 3,...)` until the desired one is connected. \n",
    "\n",
    "(Note that it is possible to choose a video file with `cv2.VideoCapture('video_file.mp4')` for testing/debugging.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d1ffde2d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if debug: \n",
    "    img_raw = cv2.cvtColor(cv2.imread('./test_images/test_{}.jpg'.format(test_name)), cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    _, img_raw = cam.read()\n",
    "\n",
    "img = cv2.resize(img_raw, viz_res, interpolation=cv2.INTER_AREA)\n",
    "img_rgba = rgba(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A2. Edge Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A2-1.** **Canny Edge Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny(img, threshold_1=70, threshold_2=70):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edge = cv2.Canny(gray, 70, 70)\n",
    "    return cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests\n",
    "edge = canny(img)\n",
    "plt.imshow(edge); plt.show()\n",
    "\n",
    "# Parameter Exploration\n",
    "# plt.imshow(canny(img, 60, 120)); plt.show()\n",
    "# plt.imshow(canny(img, 100, 200)); plt.show()\n",
    "# plt.imshow(canny(img, 200, 400)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A2-2.** **Deep Edge Detection** (PiDiNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out in favor of ONNX to save memory and improve loading time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "import torchvision.transforms as transforms\n",
    "prepare = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # imports\n",
    "# from pidinet import models\n",
    "# from pidinet.models.convert_pidinet import convert_pidinet\n",
    "# from pidinet.utils import *\n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # args\n",
    "# class Args():\n",
    "#     def __init__(self):\n",
    "#         # dynamic (likely to change)\n",
    "#         self.model = 'pidinet_small_converted'; self.evaluate_converted = True # refer to paper and source code\n",
    "#         self.evaluate = './pidinet/trained_models/table5_pidinet-small.pth'\n",
    "#         self.config = 'carv4' # model configurations\n",
    "#         # static (unlikely to change)\n",
    "#         self.sa = True; self.dil = True # use attention and dilation in diffnet respectively\n",
    "#         self.use_cuda = torch.cuda.is_available()\n",
    "#         # unused / dummy\n",
    "#         self.save_dir = './pidinet/trained_models/'\n",
    "\n",
    "# args = Args()\n",
    "\n",
    "# # model\n",
    "# model = getattr(models, args.model)(args)\n",
    "# model = nn.DataParallel(model)\n",
    "# checkpoint = load_checkpoint(args, running_file = None)\n",
    "# if args.evaluate_converted: \n",
    "#     model.load_state_dict(convert_pidinet(checkpoint['state_dict'], args.config)) # assumes CUDA\n",
    "# else: \n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# if args.use_cuda: model = torch.nn.DataParallel(model).cuda()\n",
    "# model.eval(); torch.set_grad_enabled(False) # sets the grad mode forever. source: https://discuss.pytorch.org/t/difference-between-set-grad-enabled-false-and-no-grad/72038\n",
    "\n",
    "# # results = model(prepare(img))\n",
    "# # result = torch.squeeze(results[-1]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def pidinet(img):\n",
    "#     return torch.squeeze(model(prepare(img))[-1]).cpu().numpy()\n",
    "\n",
    "# # test\n",
    "# edge = (pidinet(img)*255).astype(np.uint8)\n",
    "# plt.imshow(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2-3. **PiDiNet via ONNX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions (7/28/2022):\n",
    "- slower than expected. only option left now is TensorRT + INT8 quantization, pruning, [ONNX profiling](https://linuxtut.com/en/659b99396e63e0378f78/)\n",
    "- There may be more options than what I am aware of.\n",
    "  - Neural Magic?\n",
    "  - Refer to [Hacker News thread](https://news.ycombinator.com/item?id=28484373) and tap into collective intelligence. \n",
    "    - So what are the best practices for CPU / GPU **inference**?\n",
    "      - If CPU can be easily (within my skill / scope level) optimized, then CPU is honestly preferable.   \n",
    "      But if GPU is much faster (or can easily be utilized w/ async using ONNX), then definitely GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession('PiDiNet_Tiny_1_simple_half.onnx', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "def pidinet_onnx(img):\n",
    "    outputs = ort_sess.run(None, {'modelInput': prepare(img).numpy()})\n",
    "    return outputs[-1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d19b7f9130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "plt.imshow(pidinet_onnx(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit [ort_sess.run(None, {'modelInput': prepare(img).numpy()}) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark results (Tiny, FP16) (local): 11 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A3. Semantic Segmentation (Yolact Edge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Also available as ONNX](https://github.com/PINTO0309/PINTO_model_zoo/tree/main/221_YOLACT-PyTorch). If TensorRT local runtime environment setup is feasible (not feeling comfortable with/ WSL2 + Nvidia-Docker solution due to future requirement for Bluetooth connections), then let's apply optimization techniques to this and move away from Yolact Edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "from eval import *\n",
    "parse_args([\"--config=yolact_edge_config\", \"--calib_images=../calib_images\"])\n",
    "from eval import args\n",
    "\n",
    "# settings\n",
    "args.trained_model = \"./weights/yolact_edge_54_800000.pth\"\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "args.score_threshold = 0.5 # 0.3 # 0.15 # THRESHOLD # DEBUG # 중요!\n",
    "args.top_k = 15\n",
    "\n",
    "# initialize the network, load a pre-trained model.\n",
    "net = Yolact(training=False)\n",
    "net.load_weights(args.trained_model, args=args)\n",
    "net.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# settings 2\n",
    "net.detect.use_fast_nms = args.fast_nms\n",
    "cfg.mask_proto_debug = args.mask_proto_debug # ?\n",
    "extras = {\"backbone\": \"full\", \"interrupt\": False, \"keep_statistics\": False,  \"moving_statistics\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d2100fe4c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frame preparation\n",
    "frame = torch.from_numpy(img).cuda().float(); batch = FastBaseTransform()(frame.unsqueeze(0))\n",
    "blank = torch.from_numpy(np.zeros(shape=img.shape)).cuda().float()\n",
    "\n",
    "# segmentation\n",
    "preds = net(batch, extras=extras)[\"pred_outs\"]\n",
    "preds2 = net(batch, extras=extras)[\"pred_outs\"]\n",
    "preds3 = net(batch, extras=extras)[\"pred_outs\"]\n",
    "\n",
    "plt.imshow(prep_display(preds, frame, None, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preds[0]['box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_mask(dets_out, img): \n",
    "    img_gpu = img / 255.0\n",
    "    h, w, _ = img.shape\n",
    "    \n",
    "    with timer.env('Postprocess'):\n",
    "        t = postprocess(dets_out, w, h, visualize_lincomb = args.display_lincomb,\n",
    "                                        crop_masks        = args.crop,\n",
    "                                        score_threshold   = args.score_threshold)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    with timer.env('Copy'):\n",
    "        if cfg.eval_mask_branch:\n",
    "            # Masks are drawn on the GPU, so don't copy\n",
    "            masks = t[3][:args.top_k]\n",
    "        # classes, scores, boxes = [x[:args.top_k].cpu().numpy() for x in t[:3]]\n",
    "    \n",
    "    try:\n",
    "        masks = np.transpose(masks.cpu(), (1,2,0)).numpy()\n",
    "        masks = masks.sum(axis=2)\n",
    "        masks = np.interp(masks, (0, masks.max()), (0, 1)) # masks.min()\n",
    "        return masks\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('binary_mask:', e)\n",
    "        return np.zeros(img.shape[:2])\n",
    "\n",
    "# plt.imshow(binary_mask(preds2, frame)); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = cfg.dataset.class_names\n",
    "\n",
    "def analyze_segmentation(dets): # dets is preds[0]\n",
    "    \"\"\"returns the bounding box of classes of detected masks... but what about multi-instance cases?\"\"\"\n",
    "    info = {}\n",
    "    classes = dets['class']\n",
    "    for i in range(len(classes)):\n",
    "        info[class_names[classes[i]]] = dets['box'][i].cpu().numpy().astype(np.int32)\n",
    "\n",
    "    return info # output format: [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A3-1. Exploring `preds` Consumption**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def consume_preds(): pass # 필요 없어서 지움\n",
    "# **결론**: `postprocess`를 지나는 순간 dets 소모."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A3-2. `viz_and_mask()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def viz_and_mask(dets_out, img, h=None, w=None, undo_transform=False, class_color=True, mask_alpha=0.45):\n",
    "    \"\"\"Combination of original prep_display method & binary_mask method. Debug separately.\"\"\"\n",
    "    if undo_transform:\n",
    "        img_numpy = undo_image_transformation(img, w, h)\n",
    "        img_gpu = torch.Tensor(img_numpy).cuda()\n",
    "    else:\n",
    "        img_gpu = img / 255.0\n",
    "        h, w, _ = img.shape\n",
    "    \n",
    "    with timer.env('Postprocess'):\n",
    "        t = postprocess(dets_out, w, h, visualize_lincomb = args.display_lincomb,\n",
    "                                        crop_masks        = args.crop,\n",
    "                                        score_threshold   = args.score_threshold)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    with timer.env('Copy'):\n",
    "        if cfg.eval_mask_branch:\n",
    "            # Masks are drawn on the GPU, so don't copy\n",
    "            masks = t[3][:args.top_k]\n",
    "            mask = t[3][:args.top_k] # 짜가 # maybe need to copy this via cpu.numpy method.\n",
    "        classes, scores, boxes = [x[:args.top_k].cpu().numpy() for x in t[:3]]\n",
    "\n",
    "        # from binary_mask()\n",
    "        if mask.ndim >= 3:\n",
    "            mask = np.transpose(masks.cpu(), (1,2,0)).numpy()\n",
    "            mask = mask.sum(axis=2)\n",
    "            mask = np.interp(mask, (0, mask.max()), (0, 1)) # masks.min()\n",
    "        else: \n",
    "            mask = np.zeros(img.shape[:2])\n",
    "        \n",
    "\n",
    "    num_dets_to_consider = min(args.top_k, classes.shape[0])\n",
    "    for j in range(num_dets_to_consider):\n",
    "        if scores[j] < args.score_threshold:\n",
    "            num_dets_to_consider = j\n",
    "            break\n",
    "    \n",
    "    if num_dets_to_consider == 0:\n",
    "        # No detections found so just output the original image\n",
    "        return (img_gpu * 255).byte().cpu().numpy(), np.zeros(img.shape[:2])\n",
    "\n",
    "    # Quick and dirty lambda for selecting the color for a particular index\n",
    "    # Also keeps track of a per-gpu color cache for maximum speed\n",
    "    def get_color(j, on_gpu=None):\n",
    "        global color_cache\n",
    "        color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)\n",
    "        \n",
    "        if on_gpu is not None and color_idx in color_cache[on_gpu]:\n",
    "            return color_cache[on_gpu][color_idx]\n",
    "        else:\n",
    "            color = COLORS[color_idx]\n",
    "            if not undo_transform:\n",
    "                # The image might come in as RGB or BRG, depending\n",
    "                color = (color[2], color[1], color[0])\n",
    "            if on_gpu is not None:\n",
    "                color = torch.Tensor(color).to(on_gpu).float() / 255.\n",
    "                color_cache[on_gpu][color_idx] = color\n",
    "            return color\n",
    "\n",
    "    # First, draw the masks on the GPU where we can do it really fast\n",
    "    # Beware: very fast but possibly unintelligible mask-drawing code ahead\n",
    "    # I wish I had access to OpenGL or Vulkan but alas, I guess Pytorch tensor operations will have to suffice\n",
    "    if args.display_masks and cfg.eval_mask_branch:\n",
    "        # After this, mask is of size [num_dets, h, w, 1]\n",
    "        masks = masks[:num_dets_to_consider, :, :, None]\n",
    "        \n",
    "        # Prepare the RGB images for each mask given their color (size [num_dets, h, w, 1])\n",
    "        colors = torch.cat([get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)\n",
    "        masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha\n",
    "\n",
    "        # This is 1 everywhere except for 1-mask_alpha where the mask is\n",
    "        inv_alph_masks = masks * (-mask_alpha) + 1\n",
    "        \n",
    "        # I did the math for this on pen and paper. This whole block should be equivalent to:\n",
    "        #    for j in range(num_dets_to_consider):\n",
    "        #        img_gpu = img_gpu * inv_alph_masks[j] + masks_color[j]\n",
    "        masks_color_summand = masks_color[0]\n",
    "        if num_dets_to_consider > 1:\n",
    "            inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)\n",
    "            masks_color_cumul = masks_color[1:] * inv_alph_cumul\n",
    "            masks_color_summand += masks_color_cumul.sum(dim=0)\n",
    "\n",
    "        img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand\n",
    "        \n",
    "    # Then draw the stuff that needs to be done on the cpu\n",
    "    # Note, make sure this is a uint8 tensor or opencv will not anti alias text for whatever reason\n",
    "    img_numpy = (img_gpu * 255).byte().cpu().numpy()\n",
    "    \n",
    "    if args.display_text or args.display_bboxes:\n",
    "        for j in reversed(range(num_dets_to_consider)):\n",
    "            x1, y1, x2, y2 = boxes[j, :]\n",
    "            color = get_color(j)\n",
    "            score = scores[j]\n",
    "\n",
    "            if args.display_bboxes:\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)\n",
    "\n",
    "            if args.display_text:\n",
    "                _class = cfg.dataset.class_names[classes[j]]\n",
    "                text_str = '%s: %.2f' % (_class, score) if args.display_scores else _class\n",
    "\n",
    "                font_face = cv2.FONT_HERSHEY_DUPLEX\n",
    "                font_scale = 0.6\n",
    "                font_thickness = 1\n",
    "\n",
    "                text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]\n",
    "\n",
    "                text_pt = (x1, y1 - 3)\n",
    "                text_color = [255, 255, 255]\n",
    "\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)\n",
    "                cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        return img_numpy, mask\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('viz_and_mask:', e)\n",
    "\n",
    "# seg_overlay, seg_mask = viz_and_mask(preds3, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A3-3. Reload, One-Cycle for Acceleration @ Debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = net(batch, extras=extras)[\"pred_outs\"]; dets = preds[0]\n",
    "\n",
    "# TODO optimize : complexity of viz_and_mask\n",
    "seg_overlay, seg_mask = viz_and_mask(preds, frame)\n",
    "# plt.imshow(seg_overlay); plt.show(); plt.imshow(seg_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A4. Edge Masking (Background Removal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seg_rgba = rgba(edge, seg_mask)\n",
    "\n",
    "# tests\n",
    "# print(np.unique(seg_rgba))\n",
    "plt.imshow(seg_rgba); plt.show()\n",
    "plt.imshow(blend(rgba(edge, seg_mask))); plt.show()\n",
    "\n",
    "# both methods work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A4-1. Mask Dilation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "References:\n",
    "\n",
    "- [Dilation - Wikipedia](https://en.wikipedia.org/wiki/Dilation_%28morphology%29)\n",
    "- [Stackoverflow - NumPy Dilation (similar problem)](https://stackoverflow.com/questions/32706135/extend-numpy-mask-by-n-cells-to-the-right-for-each-bad-value-efficiently)\n",
    "- [`scipy.ndimage.binary_dilation`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.binary_dilation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_r = 5 # dilation radius\n",
    "stru = cv2.circle(np.zeros(shape=[d_r*2, d_r*2, 3]), (d_r, d_r), d_r, (255, 255, 255), -1)[:,:,0]\n",
    "# plt.imshow(stru); plt.show()\n",
    "\n",
    "# TODO this may promote brightness of all nonzero values to 1.\n",
    "def dilate_mask(mask):\n",
    "    return ndimage.binary_dilation(seg_mask, structure=stru, iterations=1, mask=None)\n",
    "\n",
    "def dilate_mask2(mask):\n",
    "    return ndimage.grey_dilation(seg_mask, footprint=stru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_mask3(mask):\n",
    "    return ndimage.grey_dilation(seg_mask, size=(d_r, d_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test - original\n",
    "masked_edge = blend(rgba(edge, seg_mask))\n",
    "# plt.imshow(masked_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tests - dilated\n",
    "# plt.imshow(seg_mask); plt.show()\n",
    "dilated = dilate_mask(seg_mask)\n",
    "# dilated2 = dilate_mask2(seg_mask)\n",
    "# plt.imshow(dilated); plt.show()\n",
    "# plt.imshow(dilated2); plt.show()\n",
    "# plt.imshow(blend(rgba(edge, dilated))); plt.show();\n",
    "# plt.imshow(blend(rgba(edge, dilated2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Canny Edge와 Deep (PiDiNet) Edge 둘다 실사용에서 비교해보자~ ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A5. Hand Tracking (MediaPipe Hand)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(model_complexity=0, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# process\n",
    "mp_results = hands.process(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FINGERS = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "MP_FINGERS = ['THUMB', 'INDEX_FINGER', 'MIDDLE_FINGER', 'RING_FINGER', 'PINKY']\n",
    "MP_KEYS = mp_hands.HandLandmark\n",
    "FINGER_COLORS = ([180, 229, 255], [128, 64, 128], [0, 204, 255], [48, 255, 48], [192, 101, 21])\n",
    "\n",
    "def tip_info(mp_results, img): # hardcoded, naive.\n",
    "    \n",
    "    pos = {}; dir = {}\n",
    "\n",
    "    # detected pose landmarks\n",
    "    landmarks = mp_results.multi_hand_landmarks\n",
    "\n",
    "    if landmarks: # i.e., if hands are detected \n",
    "        y_max, x_max = img.shape[:2]\n",
    "        # NOTE: only the lastly (most recent) detected hand is used\n",
    "        for hand in landmarks: \n",
    "            landmark = hand.landmark \n",
    "            # readable shortcuts\n",
    "            thumb_tip  = landmark[MP_KEYS.THUMB_TIP];         thumb_dip  = landmark[MP_KEYS.THUMB_IP]\n",
    "            index_tip  = landmark[MP_KEYS.INDEX_FINGER_TIP];  index_dip  = landmark[MP_KEYS.INDEX_FINGER_DIP]\n",
    "            middle_tip = landmark[MP_KEYS.MIDDLE_FINGER_TIP]; middle_dip = landmark[MP_KEYS.MIDDLE_FINGER_DIP]\n",
    "            ring_tip   = landmark[MP_KEYS.RING_FINGER_TIP];   ring_dip   = landmark[MP_KEYS.RING_FINGER_DIP]\n",
    "            pinky_tip  = landmark[MP_KEYS.PINKY_TIP];         pinky_dip  = landmark[MP_KEYS.PINKY_DIP]\n",
    "            \n",
    "            # convert normalized coordinates (0-1) to pixel coordinates.\n",
    "            pos['thumb']  = np.array((thumb_tip.x  * x_max, thumb_tip.y  * y_max), dtype=np.int32)\n",
    "            pos['index']  = np.array((index_tip.x  * x_max, index_tip.y  * y_max), dtype=np.int32)\n",
    "            pos['middle'] = np.array((middle_tip.x * x_max, middle_tip.y * y_max), dtype=np.int32)\n",
    "            pos['ring']   = np.array((ring_tip.x   * x_max, ring_tip.y   * y_max), dtype=np.int32)\n",
    "            pos['pinky']  = np.array((pinky_tip.x  * x_max, pinky_tip.y  * y_max), dtype=np.int32)\n",
    "            \n",
    "            # compute finger direction using the tip position and DIP (distal phalanx) position. \n",
    "            # NOTE: fingertip direction is based on pixel coordinates (i.e., [downwards y, rightwards x])\n",
    "            # NOTE: This is wildly inaccurate in 3D. User is expected to keep hand planar to the camera. \n",
    "            #       A proper implementation based on 3D euler angles is needed for further improvements.\n",
    "            dir['thumb']  = np.array((thumb_tip.x  - thumb_dip.x,  thumb_tip.y  - thumb_dip.y))\n",
    "            dir['index']  = np.array((index_tip.x  - index_dip.x,  index_tip.y  - index_dip.y))\n",
    "            dir['middle'] = np.array((middle_tip.x - middle_dip.x, middle_tip.y - middle_dip.y))\n",
    "            dir['ring']   = np.array((ring_tip.x   - ring_dip.x,   ring_tip.y   - ring_dip.y))\n",
    "            dir['pinky']  = np.array((pinky_tip.x  - pinky_dip.x,  pinky_tip.y  - pinky_dip.y))\n",
    "            \n",
    "            # normalize direction vectors\n",
    "            for finger in FINGERS:\n",
    "                dir[finger] /= np.linalg.norm(dir[finger])\n",
    "            \n",
    "    return pos, dir\n",
    "\n",
    "tip_pos, tip_dir = tip_info(mp_results, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize visualization\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_hands(mp_results, img):\n",
    "    img = img.copy()\n",
    "    landmarks = mp_results.multi_hand_landmarks\n",
    "    if landmarks:\n",
    "        for landmark in landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                img, landmark, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "        # print(landmarks)\n",
    "    return img\n",
    "\n",
    "# plt.imshow(visualize_hands(mp_results, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 3d visualization (source: MediaPipe Documentation)\n",
    "# if results.multi_hand_world_landmarks is not None:\n",
    "#   for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "#     mp_drawing.plot_landmarks(\n",
    "#       hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d1a1a82490>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_tips(tip_pos, tip_dir=None, img=None):\n",
    "    img = img.copy()\n",
    "    r = 20\n",
    "    for i, finger in enumerate(tip_pos):\n",
    "        cv2.circle(img, tip_pos[finger], r, FINGER_COLORS[i], 1) # 2)\n",
    "        # cv2.line(img, tip_pos[finger], (tip_pos[finger][0] + tip_dir[finger][0] * r, tip_pos[finger][1] + tip_dir[finger][1] * r), FINGER_COLORS[i], 1) # 2)\n",
    "        cv2.line(img, tip_pos[finger], (tip_pos[finger]+tip_dir[finger]*r).astype(np.int32), FINGER_COLORS[i], 1) # 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "plt.imshow(visualize_tips(*tip_info(mp_results, img), img)) # * unpacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Optimization:\n",
    "To properly conduct vibratory tactile substitution experiments, real-time framerae is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A6. TipLets Output Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def draw_tiplet(origin, magnitudes, img, custom_color=None): # mutable... probably # origin is upper-left. magnitudes is [] of floats (0-1) with length of 6.\n",
    "    # img = img.copy()\n",
    "    origin = origin.copy()\n",
    "    \n",
    "    rad = 10\n",
    "    dia = rad*2\n",
    "    m = 3 # margin\n",
    "    origin += np.array([0, 0]) # adjustment\n",
    "\n",
    "    # fill_color = (255,255,255)\n",
    "    fill_color = [255,255,255]\n",
    "    # outline_color = (0.5,0.5,0.5) # cv2 infers max threshold as 255 or 1.0. \n",
    "    # outline_color = [127,127,127]\n",
    "    outline_color = [180,180,180]\n",
    "    th = 2 # thickness\n",
    "\n",
    "    if img.shape[2] == 4: # if img==RGBA\n",
    "        fill_color.append(255)\n",
    "        outline_color.append(255)\n",
    "\n",
    "    # fill\n",
    "    cv2.circle(img, origin+np.array([     0,         0]), int(magnitudes[0]*rad), fill_color, -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,         0]), int(magnitudes[1]*rad), fill_color, -1)\n",
    "    cv2.circle(img, origin+np.array([     0,     dia+m]), int(magnitudes[2]*rad), fill_color, -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,     dia+m]), int(magnitudes[3]*rad), fill_color, -1)\n",
    "    cv2.circle(img, origin+np.array([     0, (dia+m)*2]), int(magnitudes[4]*rad), fill_color, -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m, (dia+m)*2]), int(magnitudes[5]*rad), fill_color, -1)\n",
    "\n",
    "    # outline\n",
    "    cv2.circle(img, origin+np.array([     0,         0]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,         0]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([     0,     dia+m]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,     dia+m]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([     0, (dia+m)*2]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m, (dia+m)*2]), rad, outline_color, th)\n",
    "\n",
    "    return img\n",
    "\n",
    "# plt.imshow(draw_tiplet(origin=np.array([20, 20]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[100,100,3]))); plt.show()\n",
    "# plt.imshow(draw_tiplet(origin=np.array([20, 20]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[100,100,4]))); plt.show()\n",
    "# plt.imshow(draw_tiplet(origin=np.array([10, 10]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[67,44,3])))\n",
    "# plt.imshow(draw_tiplet(origin=np.array([10, 10]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[67,44,3], dtype=np.uint8)))\n",
    "# plt.imshow(draw_tiplet(origin=np.array([20, 20]), magnitudes=[1, 1, 1, 1, 1, 1], img=np.zeros(shape=img.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d1a1af8a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_tiplet2(origin, magnitudes, img, custom_color=None): # mutable... probably # origin is upper-left. magnitudes is [] of floats (0-1) with length of 6.\n",
    "    # img = img.copy()\n",
    "    origin = origin.copy()\n",
    "    \n",
    "    rad = 10\n",
    "    dia = rad*2\n",
    "    m = 3 # margin\n",
    "    origin += np.array([0, 0]) # adjustment\n",
    "\n",
    "    if custom_color: fill_color = custom_color.copy()\n",
    "    else:            fill_color = [255,255,255]\n",
    "    # outline_color = (0.5,0.5,0.5) # cv2 infers max threshold as 255 or 1.0. \n",
    "    # outline_color = [127,127,127]\n",
    "    outline_color = [180,180,180]\n",
    "    # outline_color = [220,220,220]\n",
    "    th = 2 # thickness\n",
    "\n",
    "\n",
    "    if img.shape[2] == 4: # if img==RGBA\n",
    "        # fill_color.append(255)\n",
    "        outline_color.append(255)\n",
    "\n",
    "\n",
    "    fill_color = np.array(fill_color, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    # fill\n",
    "    cv2.circle(img, origin+np.array([     0,         0]), rad, (fill_color*magnitudes[0]).astype(np.uint8).tolist()+[255], -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,         0]), rad, (fill_color*magnitudes[1]).astype(np.uint8).tolist()+[255], -1)\n",
    "    cv2.circle(img, origin+np.array([     0,     dia+m]), rad, (fill_color*magnitudes[2]).astype(np.uint8).tolist()+[255], -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,     dia+m]), rad, (fill_color*magnitudes[3]).astype(np.uint8).tolist()+[255], -1)\n",
    "    cv2.circle(img, origin+np.array([     0, (dia+m)*2]), rad, (fill_color*magnitudes[4]).astype(np.uint8).tolist()+[255], -1)\n",
    "    cv2.circle(img, origin+np.array([ dia+m, (dia+m)*2]), rad, (fill_color*magnitudes[5]).astype(np.uint8).tolist()+[255], -1)\n",
    "\n",
    "    # outline\n",
    "    cv2.circle(img, origin+np.array([     0,         0]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,         0]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([     0,     dia+m]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m,     dia+m]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([     0, (dia+m)*2]), rad, outline_color, th)\n",
    "    cv2.circle(img, origin+np.array([ dia+m, (dia+m)*2]), rad, outline_color, th)\n",
    "\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "# plt.imshow(draw_tiplet2(origin=np.array([20, 20]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[100,100,3]))); plt.show()\n",
    "# plt.imshow(draw_tiplet2(origin=np.array([20, 20]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[100,100,4]))); plt.show()\n",
    "plt.imshow(draw_tiplet2(origin=np.array([10, 10]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[67,44,3])))\n",
    "# plt.imshow(draw_tiplet2(origin=np.array([20, 20]), magnitudes=[1, 1, 1, 1, 1, 1], img=np.zeros(shape=img.shape, dtype=np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiplet_viz = draw_tiplet(origin=np.array([10, 10]), magnitudes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], img=np.zeros(shape=[67,44,3], dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiplet_viz.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A6-1. TipLets Overview Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d21b6a3970>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_tiplet_outputs(outputs):\n",
    "    img = np.zeros(shape=[100, 400, 3], dtype=np.uint8)\n",
    "    origin = np.array([25, 20])\n",
    "    spacing = np.array([80, 0])\n",
    "    draw_tiplet2(origin,           outputs['thumb'],  img)\n",
    "    draw_tiplet2(origin+spacing,   outputs['index'],  img)\n",
    "    draw_tiplet2(origin+spacing*2, outputs['middle'], img)\n",
    "    draw_tiplet2(origin+spacing*3, outputs['ring'],   img)\n",
    "    draw_tiplet2(origin+spacing*4, outputs['little'], img)\n",
    "    return img\n",
    "\n",
    "outputs = {'thumb': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "           'index': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "           'middle': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "           'ring': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "           'little': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}\n",
    "plt.imshow(draw_tiplet_outputs(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A6-2. On-Screen Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn onto `virtual_fov` window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# graphics utils\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "# Source:   https://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n",
    "# Source 2: https://www.adamsmith.haus/python/answers/how-to-get-the-angle-between-two-vectors-in-python\n",
    "# Modified to add directionality for vector alignment.\n",
    "\n",
    "def angle_between_2d(v1=(0,1), v2=(1,0)):\n",
    "    \n",
    "    u1 = v1 / np.linalg.norm(v1)\n",
    "    u2 = v2 / np.linalg.norm(v2)\n",
    "    dot_product = np.dot(u1, u2)\n",
    "\n",
    "    angle = np.arccos(dot_product)*57.3 # in degrees\n",
    "    \n",
    "    if np.cross(u1, u2) < 0: return -angle\n",
    "    else: return angle\n",
    "\n",
    "# tests\n",
    "# print(angle_between_2d(v1=(0.1,1), v2=(1,0)))\n",
    "# print(angle_between_2d(v1=(0.1,-1), v2=(1,0)))\n",
    "# print(angle_between_2d(v1=(0,1), v2=(1,0)))\n",
    "# print(angle_between_2d(v1=(1,0), v2=(1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Unit Canvas 크기:\n",
    "# - shape=[67,44,4]가 현재 tiplets에 딱 맞는 사이즈지만 회전을 고려하면 80x80 필요. \n",
    "# - 만약 80x80이라면 origin을 무엇으로 잡을까? \n",
    "# - [10,10] + [(80-67), (80-44)]/2 = [16.5, 28] in (x, y) format. \n",
    "# - OpenCV circle은 round-down 하는 경향이 있기에 16.5 -> 16\n",
    "\n",
    "# def draw_tiplet_at(values, center, direction, img, scale=0.5, custom_color=None): # center -> (y, x)\n",
    "#     # unit canvas\n",
    "#     canvas = np.zeros(shape=[80,80,4], dtype=np.uint8) # \n",
    "#     # draw\n",
    "#     # draw_tiplet(np.array([28, 16]), values, canvas, custom_color=custom_color)\n",
    "#     draw_tiplet2(np.array([28, 16]), values, canvas, custom_color=custom_color)\n",
    "#     # rotate\n",
    "#     angle = angle_between_2d(direction, np.array([0, -1])) # direction vector to rotation angle *with directionality*\n",
    "#     M = cv2.getRotationMatrix2D((40,40), angle, scale)\n",
    "#     # resize\n",
    "#     canvas = cv2.warpAffine(canvas, M, canvas.shape[:2], flags=cv2.INTER_AREA)\n",
    "#     # integrate\n",
    "#     big_canvas = np.zeros(shape=(*img.shape[:2],4), dtype=np.uint8)\n",
    "#     big_canvas[center[1]-40:center[1]+40, center[0]-40:center[0]+40] = canvas\n",
    "#     # NOTE tip_info()가 (x,y) 식의 좌표를 줘서 약간 헷갈리는데, 바꾸는게 좋을지도. 생각해보자.\n",
    "\n",
    "#     return blendtwo2(big_canvas, img)\n",
    "\n",
    "\n",
    "def draw_tiplet_at2(values, center, direction, img, scale=0.5, custom_color=None): # center -> (y, x)\n",
    "    # unit canvas\n",
    "    canvas = np.zeros(shape=[80,80,3], dtype=np.uint8) # \n",
    "    # draw\n",
    "    draw_tiplet2(np.array([28, 16]), values, canvas, custom_color=custom_color)\n",
    "    # rotate\n",
    "    angle = angle_between_2d(direction, np.array([0, -1])) # direction vector to rotation angle *w/ directionality*\n",
    "    M = cv2.getRotationMatrix2D((40,40), angle, scale)\n",
    "    # resize\n",
    "    canvas = cv2.warpAffine(canvas, M, canvas.shape[:2], flags=cv2.INTER_AREA)\n",
    "    # integration with output image is done @ visualize_tiplets2(), which calls this function.\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# test\n",
    "# plt.imshow(draw_tiplet_at([1,0.5,0.5,1,1,0.5], np.array([100,100]), np.array([0.5,0.5]), img, 0.5)); plt.show()\n",
    "# plt.imshow(draw_tiplet_at([1,0.5,0.5,1,1,0.5], np.array([100,100]), np.array([0.5,0.5]), np.zeros(shape=img.shape), 0.5)); plt.show()\n",
    "# plt.imshow(draw_tiplet_at([1,0.8,0.6,0.4,0.2,0], np.array([100,100]), np.array([1,0]), np.zeros(shape=img.shape), 0.5)); plt.show()\n",
    "# plt.imshow(draw_tiplet_at([1,0.8,0.6,0.4,0.2,0], np.array([100,100]), np.array([0,1]), np.zeros(shape=img.shape), 0.5)); plt.show()\n",
    "# plt.imshow(draw_tiplet_at([1,0.8,0.6,0.4,0.2,0], np.array([100,100]), np.array([-1,0]), np.zeros(shape=img.shape), 0.5)); plt.show()\n",
    "# plt.imshow(draw_tiplet_at([1,0.8,0.6,0.4,0.2,0], np.array([100,100]), np.array([0,-1]), np.zeros(shape=img.shape), 0.5)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `visualize_tiplets2` returns flawless results but is marginally slower (~5% framerate drop). `visualize_tiplets3` is faster but shows distortions on overlapped regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'thumb'\n"
     ]
    }
   ],
   "source": [
    "# def visualize_tiplets(tip_pos, tip_dir, img):\n",
    "#     for (i, finger) in enumerate(FINGERS):\n",
    "#         img = draw_tiplet_at([1, 0.8, 0.6, 0.4, 0.2, 0], tip_pos[finger], tip_dir[finger], img, custom_color=FINGER_COLORS[i])\n",
    "#     return img\n",
    "\n",
    "def visualize_tiplets2(tip_pos, tip_dir, img):\n",
    "\n",
    "    big_canvas = np.zeros(shape=img.shape, dtype=np.uint8) # shape=(*img.shape[:2],3) if input image is RGBA\n",
    "    \n",
    "    for (i, finger) in enumerate(FINGERS):\n",
    "        center, direction = tip_pos[finger], tip_dir[finger]\n",
    "\n",
    "        canvas = draw_tiplet_at2([1, 0.8, 0.6, 0.4, 0.2, 0], center, direction, img, custom_color=FINGER_COLORS[i])\n",
    "\n",
    "        # big_canvas[center[1]-40:center[1]+40, center[0]-40:center[0]+40] += canvas\n",
    "        nonzero=(big_canvas==0) # array of bool\n",
    "        candidate = big_canvas.copy() \n",
    "        candidate[center[1]-40:center[1]+40, center[0]-40:center[0]+40] = canvas\n",
    "\n",
    "        big_canvas[nonzero] = candidate[nonzero]\n",
    "\n",
    "    return big_canvas\n",
    "\n",
    "\n",
    "def visualize_tiplets3(tip_pos, tip_dir, img):\n",
    "\n",
    "    big_canvas = np.zeros(shape=(*img.shape[:2],3), dtype=np.uint8)\n",
    "    \n",
    "    for (i, finger) in enumerate(FINGERS):\n",
    "        center, direction = tip_pos[finger], tip_dir[finger]\n",
    "\n",
    "        canvas = draw_tiplet_at2([1, 0.8, 0.6, 0.4, 0.2, 0], center, direction, img, custom_color=FINGER_COLORS[i])\n",
    "\n",
    "        big_canvas[center[1]-40:center[1]+40, center[0]-40:center[0]+40] += canvas\n",
    "\n",
    "        # basic idea: prevent overlaps by using indexing like visualize_tiplets2 but to canvas and not big_canvas TODO\n",
    "\n",
    "    # return np.clip(big_canvas, a_max=255) ## uncomment in case of error / overflow \n",
    "    return big_canvas\n",
    "\n",
    "# test\n",
    "try: plt.imshow(visualize_tiplets3(*tip_info(mp_results, img), img)); plt.show()\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: no hands detected (probably)\n"
     ]
    }
   ],
   "source": [
    "# test with hand tracking\n",
    "try:\n",
    "    # tiplets\n",
    "    plt.imshow(visualize_tiplets2(*tip_info(mp_results, img), img)); plt.show()\n",
    "    plt.imshow(visualize_tiplets2(*tip_info(mp_results, img), img)); plt.show()\n",
    "    plt.imshow(visualize_tiplets2(*tip_info(mp_results, img), np.zeros(shape=img.shape, dtype=np.uint8))); plt.show()\n",
    "\n",
    "    # # fingertip, tiplets 둘다\n",
    "    # plt.imshow(visualize_tiplets(*tip_info(mp_results, img), visualize_tips(*tip_info(mp_results, img), img))); plt.show()\n",
    "    # plt.imshow(visualize_tiplets(*tip_info(mp_results, img), visualize_tips(*tip_info(mp_results, img), np.zeros(shape=img.shape))))\n",
    "\n",
    "except Exception: \n",
    "    print('error: no hands detected (probably)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **A7. Natural Language Interactivity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "\n",
    "1. Open [the server](https://colab.research.google.com/drive/1HyD_N1ebXq6YCEvRykSOl1rt0h_dVZ9K?usp=sharing) and spin it up @ Google Colab.\n",
    "    - The server runs an image & language processing model called OFA. Learn more ([GitHub]()), ([Paper]()) \n",
    "    - Make sure GPU instance is being used (Menu-Runtime-Change Runtime Type)\n",
    "2. Copy the URL printed at the bottom of Colab notebook into the GUI\n",
    "    - Alternatively, edit ngrok_url.txt in this directory. Make sure the address ends with / (slash)\n",
    "    - The URL looks like this: http://9fb9-34-105-0-120.ngrok.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A7-1. Naive Search** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated. refer to main_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A7-2. OFA Inference via Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code quality is really not good here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://9fb9-34-105-0-120.ngrok.io/\n"
     ]
    }
   ],
   "source": [
    "url_file = open('./ngrok_url.txt', 'r+')\n",
    "\n",
    "url = url_file.read()\n",
    "if url[-1] != '/': url += '/'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connection Status Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(mode=''):\n",
    "    ping = requests.get(url).text\n",
    "    if ping == '{\"hello\":\"world\"}': \n",
    "        dpg.set_value('colab_status_text', 'Connected')\n",
    "        dpg.configure_item('colab_status_text', color=[0,255,0])\n",
    "    else:\n",
    "        dpg.set_value('colab_status_text', 'Disconnected')\n",
    "        dpg.configure_item('colab_status_text', color=[255,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL Management (via GUI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_url():\n",
    "    \n",
    "    new_url = dpg.get_value('url_input')\n",
    "    if new_url[-1] != '/': new_url += '/'\n",
    "\n",
    "    global url\n",
    "    url = new_url\n",
    "\n",
    "    url_file.seek(0)\n",
    "    url_file.write(new_url)\n",
    "    url_file.truncate()\n",
    "    url_file.close()\n",
    "\n",
    "    update_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction with OFA Server (on Google Colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ofa(input, route=''):\n",
    "\n",
    "    _, buffer = cv2.imencode('.jpg', img)\n",
    "    image_file = BytesIO(buffer)\n",
    "\n",
    "    params = { 'instruction': input }\n",
    "    # files = { 'file': open('./test_images/test_desk.jpg', 'rb') } # DEBUG\n",
    "    files = { 'file': ('img.jpeg', image_file, 'image/jepg') }\n",
    "    response = requests.post(url+route, params=params, files=files)\n",
    "\n",
    "    # print(response.text) # DEBUG\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# call_ofa('a monitor')\n",
    "# call_ofa('a screen')\n",
    "# np.array(json.loads('[436.4898231564899,314.55455455455456,569.7430764097431,360.04004004004]'), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bounding box visualization test\n",
    "# [272, 196, 356, 225]\n",
    "# ofa = cv2.rectangle(img.copy(), [272, 196], [356, 225], (0,0,255), 2)\n",
    "# plt.imshow(ofa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofa = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_language(mode='test', test_input=''):\n",
    "\n",
    "    if mode != 'test': input = dpg.get_value('language_input')\n",
    "    if mode == 'test': input = test_input\n",
    "\n",
    "    if dpg.get_value('ofa_task') == 'Object Finding':\n",
    "\n",
    "        message = call_ofa(input, 'grounding') # img is global so no need to pass.\n",
    "\n",
    "        if re.search('[a-zA-Z]', message): \n",
    "            print(message)\n",
    "        else: # only numbers (i.e., no alphabet in return message)\n",
    "            response = json.loads(message)\n",
    "            print('response:', response)\n",
    "            bbox = np.array(response, dtype=np.int32)\n",
    "            print('detected a bounding box from msg:', bbox)\n",
    "        \n",
    "            global ofa\n",
    "            try: \n",
    "                ofa = cv2.rectangle(img.copy(), [bbox[0], bbox[1]], [bbox[2], bbox[3]], (0,255,0), 2)\n",
    "                # ofa = cv2.rectangle(img.copy(), a, (0,255,0), 2)\n",
    "            except Exception as e: print('Error drawing bounding box:', e)\n",
    "\n",
    "    if dpg.get_value('ofa_task') == 'Question Answering' or dpg.get_value('ofa_task') == 'Image Captioning':\n",
    "        if dpg.get_value(\"ofa_task\") == \"Image Captioning\": input = \"What does the image describe?\" # TODO better code\n",
    "\n",
    "        message = call_ofa(input, 'answering') # img is global so no need to pass.\n",
    "\n",
    "        # if re.search('[<,>]', message): # contains word 'ngrok'. probably an http error\n",
    "            # print('error (probably):', message)\n",
    "        # else: \n",
    "        response = json.loads(message)\n",
    "        print('response:', response)\n",
    "\n",
    "        \n",
    "        try: \n",
    "            dpg.set_value('ofa_output', \"Output: \"+response)\n",
    "        except Exception as e: print('Error setting OFA output text:', e)\n",
    "        # TODO add back the regex search if empty-input VQA case (error detection false positive) successfully avoids it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_language(mode='test', test_input='the apple logo')\n",
    "# plt.imshow(ofa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **A7-3. Zoom & Re-Center**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def zoom_image(img, factor, point=None): # point -> (y,x)\n",
    "    # create a larger image. this will then be cropped into original image's size to create zoom effect.\n",
    "    large = cv2.resize(img, [int(img.shape[1]*factor), int(img.shape[0]*factor)])\n",
    "    center = (large.shape[0]/2, large.shape[1]/2)\n",
    "    # without a specified zooming point, defaults to zooming about the center\n",
    "    if point is None: upper_right = (int(center[0]-img.shape[0]/2), int(center[1]-img.shape[1]/2))\n",
    "    else:             upper_right = (int(center[0]-img.shape[0]+point[0]), int(center[1]-img.shape[1]+point[1]))\n",
    "\n",
    "    final_image = large[upper_right[0]:upper_right[0]+img.shape[0], upper_right[1]:upper_right[1]+img.shape[1]]    \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def move_center(img, center): # center -> (y,x)\n",
    "    # Translation distances in x and y axis\n",
    "\n",
    "    x_trans = int(img.shape[0]//2-center[0])\n",
    "    y_trans = int(img.shape[1]//2-center[1])\n",
    "\n",
    "    # Pad and remove pixels from image to perform translation\n",
    "\n",
    "    if x_trans > 0:\n",
    "        im2 = np.pad(img, ((x_trans, 0), (0, 0), (0, 0)), mode='constant')\n",
    "        im2 = im2[:img.shape[0]-x_trans, :]\n",
    "    else:\n",
    "        im2 = np.pad(img, ((0, -x_trans), (0, 0), (0, 0)), mode='constant')\n",
    "        im2 = im2[-x_trans:, :, :]\n",
    "\n",
    "    if y_trans > 0:\n",
    "        im3 = np.pad(im2, ((0, 0), (y_trans, 0), (0, 0)), mode='constant')\n",
    "        im3 = im3[:, :img.shape[0]-y_trans]\n",
    "\n",
    "    else:\n",
    "        im3 = np.pad(im2, ((0, 0), (0, -y_trans), (0, 0)), mode='constant')\n",
    "        im3 = im3[:, -y_trans:]\n",
    "    \n",
    "    return im3\n",
    "\n",
    "# NOTE Only works for 3-dimensional images (i.e., no grayscale). \n",
    "#      Delete the last (0,0) from pad_width argument for 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# canvas = np.zeros((200, 200, 3), dtype=np.uint8)\n",
    "# canvas = cv2.circle(canvas, (150, 150), 5, (255,0,0), 1)\n",
    "# plt.imshow(canvas); plt.show(); # circle @ bottom-right\n",
    "# plt.imshow(zoom_image(move_center(canvas, center=[150,150]), 1)) # circle @ center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **A8. Object Tracking**\n",
    "\n",
    "It would have been preferable to use [single object tracking](https://arxiv.org/ftp/arxiv/papers/2201/2201.13066.pdf) methods since it almost perfectly aligns with the purposes of this project. However, there isn't a well-implemented real-time single object tracking solution available for public. So the plan is to first use multi-object tracking (SORT), and then seek improvements if needed after conducting experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A8-0. Yolov7\n",
    "\n",
    "SORT family of multi-object tracking (MOT) algorithms require a object detection backbone. [Source](https://github.com/ibaiGorordo/ONNX-YOLOv7-Object-Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from YOLOv7 import YOLOv7\n",
    "\n",
    "model_path = './YOLOv7/models/yolov7-tiny_256x320.onnx'\n",
    "yolov7_detector = YOLOv7(model_path, conf_thres=0.3, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d21fb621c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxes, scores, class_ids = yolov7_detector(img)\n",
    "\n",
    "yolo_viz = yolov7_detector.draw_detections(img)\n",
    "plt.imshow(yolo_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A8-1. SORT\n",
    "\n",
    "SORT algorithm determines the continuity of previously & currently seen objects by applying a kalman filter. [More about SORT](). [Source](https://github.com/abewley/sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort import *\n",
    "\n",
    "#create instance of SORT\n",
    "mot_tracker = Sort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get detections\n",
    "dets = np.concatenate((boxes, scores[:,np.newaxis]),axis=1)\n",
    "\n",
    "# update SORT\n",
    "track_bbs_ids = mot_tracker.update(dets)\n",
    "# dets -> a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
    "\n",
    "# track_bbs_ids is a np array where each row contains a valid bounding box and track_id (last column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 5), dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_bbs_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A8-2. Deep SORT\n",
    "\n",
    "Deep SORT builds upon SORT by utilizing appearance features to associate objects (in addition to kalman filter), improving robustness against occlusions and out-of-sight moments. Implementing Deep SORT requires an additional appearance descriptor network (ex. OSNet); until sufficient optimization is done, stay away from this. Note: StrongSORT is a newer variation of Deep SORT which improves performance without significant computational cost. [Learn more about Deep SORT](https://wansook0316.github.io/ds/dl/2021/03/14/computer-vision-17-DeepSort.html).\n",
    "\n",
    "Brainstorming:\n",
    "- Example Deep SORT implementations\n",
    "  - (there are a plenty of them on GitHub) \n",
    "  - \n",
    "\n",
    "[Source](https://github.com/abewley/sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A9. Depth Estimation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **B. DearPyGUI Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **B.1. Viewport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup DearPyGUI\n",
    "dpg.create_context()\n",
    "dpg.create_viewport(title='Real-Time Scene Analysis', width=window_size[0], height=window_size[1])\n",
    "dpg.setup_dearpygui()\n",
    "dpg.show_viewport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **B.2. Textures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# texture registry\n",
    "with dpg.texture_registry(show=False): # show=True\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"original\")\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"edge\")\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"segmentation\")\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgba, tag=\"masked_edge\") # note rgbA\n",
    "    # dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"masked_edge\") # fallback from RGBA for batch processing\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"hand_tracking\")\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"virtual_fov\")\n",
    "    dpg.add_raw_texture(width=img.shape[1], height=img.shape[0], default_value=imdpg2(img), format=dpg.mvFormat_Float_rgb, tag=\"ofa\")\n",
    "    dpg.add_raw_texture(width=400, height=100, default_value=imdpg2(np.zeros(shape=[100,400,3])), format=dpg.mvFormat_Float_rgb, tag=\"tiplets\")\n",
    "\n",
    "# for initialization, just use original image to avoid errors. good. ... or not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **B.3. Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# row 1\n",
    "with dpg.window(label=\"Original\", tag=\"original_window\"):\n",
    "    dpg.add_image(\"original\")\n",
    "dpg.set_item_pos(\"original_window\", (0, 0))\n",
    "\n",
    "with dpg.window(label=\"Edge\", tag=\"edge_window\"):\n",
    "    dpg.add_image(\"edge\")\n",
    "dpg.set_item_pos(\"edge_window\", (20+viz_res[0], 0))\n",
    "\n",
    "# virtual FOV live-view window # column 3\n",
    "with dpg.window(label=\"Virtual FOV\", tag=\"virtual_fov_window\"):\n",
    "    dpg.add_image(\"virtual_fov\")\n",
    "    dpg.add_text(\"Zoom Factor: None\", tag=\"zoom_factor_text\")\n",
    "dpg.set_item_pos(\"virtual_fov_window\", ((20+viz_res[0])*2,0))\n",
    "\n",
    "\n",
    "# row 2\n",
    "with dpg.window(label=\"Segmentation\", tag=\"segmentation_window\"):\n",
    "    dpg.add_image(\"segmentation\")\n",
    "dpg.set_item_pos(\"segmentation_window\", (0, 40+viz_res[1]))\n",
    "\n",
    "with dpg.window(label=\"Masked Edge\", tag=\"masked_edge_window\"):\n",
    "    dpg.add_image(\"masked_edge\")\n",
    "dpg.set_item_pos(\"masked_edge_window\", (20+viz_res[0], 40+viz_res[1]))\n",
    "\n",
    "with dpg.window(label=\"Hand Tracking\", tag=\"hand_tracking_window\"):\n",
    "    dpg.add_image(\"hand_tracking\")\n",
    "dpg.set_item_pos(\"hand_tracking_window\", ((20+viz_res[0])*2, 40+viz_res[1]+22))\n",
    "\n",
    "\n",
    "# row 3\n",
    "# natural language input window\n",
    "# with dpg.window(label=\"Language Input (Naive)\", tag=\"naive_input_window\", width=15+viz_res[0]):\n",
    "#     with dpg.group(horizontal=True):\n",
    "#         dpg.add_input_text(tag=\"naive_input\")\n",
    "#         dpg.add_button(label=\"Select\", callback=select_object)\n",
    "# dpg.set_item_pos(\"naive_input_window\", (0, (40+viz_res[1])*2))\n",
    "virtual_fov = np.zeros(shape=img.shape, dtype=np.uint8)\n",
    "focus = None\n",
    "# OFA output window (temporary) TODO combine with tracking visualization (paths)\n",
    "with dpg.window(label=\"Language Input\", tag=\"language_input_window\", width=15+viz_res[0]):\n",
    "    dpg.add_text(\"Task Selection:\")\n",
    "    dpg.add_radio_button(items=['Object Finding', 'Image Captioning', 'Question Answering'], tag=\"ofa_task\", default_value='Object Finding')\n",
    "    with dpg.group(horizontal=True):\n",
    "        dpg.add_input_text(tag=\"language_input\")\n",
    "        dpg.add_button(label=\"Submit\", callback=parse_language)\n",
    "    dpg.add_text(\"Note: For image captioning task, input text is ignored.\", color=[127]*3)\n",
    "    # todo: add speech recognition button (possible because it's retained mode GUI)\n",
    "dpg.set_item_pos(\"language_input_window\", (0, (40+viz_res[1])*2))\n",
    "\n",
    "with dpg.window(label=\"Language Output\", tag=\"ofa_output_window\"):\n",
    "    dpg.add_image(\"ofa\")\n",
    "    dpg.add_text(tag=\"ofa_output\", default_value=\"\")\n",
    "dpg.set_item_pos(\"ofa_output_window\", (20+viz_res[0], (40+viz_res[1])*2))\n",
    "\n",
    "\n",
    "with dpg.window(label=\"TipLets Output\", tag=\"tiplets_output_window\"):\n",
    "    dpg.add_image(\"tiplets\")\n",
    "    dpg.add_text(\"Source of Vibration\")\n",
    "    dpg.add_radio_button(items=['Masked Edge', 'Virtual FOV'], tag=\"source\", default_value='Virtual FOV')\n",
    "\n",
    "dpg.set_item_pos(\"tiplets_output_window\", ((20+viz_res[0])*2, (40+viz_res[1])*2+22))\n",
    "# colab diagnostics\n",
    "with dpg.window(label=\"Colab Webserver Connection Diagnostics\", tag=\"diagnostics_window\", width=15+viz_res[0]):\n",
    "    with dpg.group(horizontal=True):\n",
    "        dpg.add_input_text(tag=\"url_input\", default_value=url)\n",
    "        dpg.add_button(label=\"Update URL\", callback=update_url)\n",
    "    with dpg.group(horizontal=True):\n",
    "        dpg.add_text('???', tag=\"colab_status_text\", color=[255,255,255])\n",
    "        dpg.add_button(label=\"Refresh Status\", callback=update_status)\n",
    "\n",
    "dpg.set_item_pos(\"diagnostics_window\", ((20+viz_res[0])*2, (40+viz_res[1])*2+230))\n",
    "\n",
    "\n",
    "\n",
    "# misc.\n",
    "# colormap -- maybe useful for even edge vizualization.\n",
    "# colormap = plt.get_cmap()\n",
    "# colormap = plt.get_cmap('inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **C. DPG Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23256/3716496325.py\u001b[0m in \u001b[0;36mparse_language\u001b[1;34m(mode, test_input)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# print('error (probably):', message)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'response:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hocbu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hocbu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hocbu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: a computer monitor in a room description automatically generated\n",
      "response: the interior of the building where the incident took place. photo: facebook\n",
      "response: [279.7464130797464, 58.45845845845846, 309.1091091091091, 219.4194194194194]\n",
      "detected a bounding box from msg: [279  58 309 219]\n",
      "response: [332.0653987320654, 26.426426426426424, 356.08942275608945, 54.054054054054056]\n",
      "detected a bounding box from msg: [332  26 356  54]\n",
      "response: [232.7660994327661, 66.46646646646647, 266.93360026693364, 221.021021021021]\n",
      "detected a bounding box from msg: [232  66 266 221]\n",
      "response: [172.4391057724391, 86.48648648648648, 227.42742742742743, 124.52452452452451]\n",
      "detected a bounding box from msg: [172  86 227 124]\n",
      "response: [0.0, 0.0, 399.8665331998665, 299.8998998998999]\n",
      "detected a bounding box from msg: [  0   0 399 299]\n",
      "response: [27.227227227227228, 228.62862862862863, 230.09676343009676, 299.8998998998999]\n",
      "detected a bounding box from msg: [ 27 228 230 299]\n",
      "Frames Per Second: 8.3\n",
      " \n",
      "*** Profile stats marshalled to file 'mainloop_profile.pstat'. \n",
      "\n",
      "*** Profile printout saved to text file 'mainloop_profile.txt'. \n"
     ]
    }
   ],
   "source": [
    "%%prun -s cumulative -q -l 10 -T mainloop_profile.txt -D mainloop_profile.pstat \n",
    "# profile cell & save to \"mainloop3.txt\", \"mainloop3.pstat\" for run-time analysis \n",
    "# source: https://ipython-books.github.io/42-profiling-your-code-easily-with-cprofile-and-ipython/\n",
    "\n",
    "\n",
    "c = 0; start_time = time.time()\n",
    "\n",
    "while dpg.is_dearpygui_running():\n",
    "    \n",
    "    \"\"\"Camera Input\"\"\"\n",
    "    if debug: pass\n",
    "    else: \n",
    "        _, raw = cam.read(); raw = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(raw, viz_res, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \"\"\"Canny Edge Detection (OpenCV)\"\"\"\n",
    "    # edge = canny(img)\n",
    "\n",
    "    \"\"\"Deep Learning Based Edge Detection (PiDiNet)\"\"\"\n",
    "    # edge = (pidinet(img)*255).astype(np.uint8)\n",
    "    # edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)\n",
    "    \"\"\"(via ONNX)\"\"\"\n",
    "    edge = (pidinet_onnx(img)*255).astype(np.uint8)\n",
    "    edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    \"\"\"Semantic Segmentation (Yolact Edge)\"\"\"\n",
    "    if debug: pass\n",
    "    else: \n",
    "        frame = torch.from_numpy(img).cuda().float(); batch = FastBaseTransform()(frame.unsqueeze(0)) # 만약 오작동시 BGR로 바꿔 테스트.\n",
    "        blank = torch.from_numpy(np.zeros(shape=img.shape)).cuda().float()\n",
    "        # blank = torch.from_numpy(np.zeros(shape=img.shape)).float().cuda() # any significant speed differences?\n",
    "\n",
    "        preds = net(batch, extras=extras)[\"pred_outs\"]; dets = preds[0]\n",
    "\n",
    "        seg_overlay, seg_mask = viz_and_mask(preds, frame)\n",
    "\n",
    "    \"\"\"Masked Edge\"\"\"\n",
    "    # masked_edge = rgba(edge, seg_mask)\n",
    "    masked_edge = rgba(edge, dilate_mask3(seg_mask)) \n",
    "\n",
    "    \"\"\"Hand Tracking\"\"\"\n",
    "    mp_results = hands.process(img)\n",
    "    hand_tracking = visualize_hands(mp_results, img)\n",
    "\n",
    "    \"\"\"Natural Language Interaction\"\"\"\n",
    "    # implemented as callback functions. Refer to Section A1 (i.e., parse_language(), call_ofa(), update_url())\n",
    "\n",
    "    \"\"\"Virtual FOV\"\"\"\n",
    "    # TODO wrap this into a function. # will also make it easier to debug (outside of DPG)\n",
    "    if focus is not None:\n",
    "        # virtual_fov[focus[1]:focus[3], focus[0]:focus[2]] = masked_edge[:,:,:3][focus[1]:focus[3], focus[0]:focus[2]]\n",
    "        virtual_fov[focus[1]:focus[3], focus[0]:focus[2]] = blend(masked_edge)[focus[1]:focus[3], focus[0]:focus[2]]\n",
    "        # print(virtual_fov.shape); print(img.shape)\n",
    "        margin = 10\n",
    "        zoom_factor = min((img.shape[0]-margin)/(focus[3]-focus[1]), (img.shape[1]-margin)/(focus[2]-focus[0]))\n",
    "        focus_center = [(focus[3]+focus[1])//2, (focus[2]+focus[0])//2] # (y,x)\n",
    "        # virtual_fov = zoom_image(virtual_fov, zoom_factor, focus_center)\n",
    "        virtual_fov = zoom_image(move_center(virtual_fov, focus_center), zoom_factor) \n",
    "\n",
    "        dpg.set_value('zoom_factor_text', \"Zoom Factor: {}\".format(round(zoom_factor, 2)))\n",
    "        \n",
    "        # virtual_fov[focus[1]:focus[3], focus[0]:focus[2]] = 255 # white-mask virtual box\n",
    "        # masked_edge[focus[1]:focus[3], focus[0]:focus[2]][:3]\n",
    "    else: pass\n",
    "\n",
    "\n",
    "    \"\"\"TipLets Output\"\"\"\n",
    "    tip_pos, tip_dir = tip_info(mp_results, img)\n",
    "    tiplets = draw_tiplet_outputs(outputs)\n",
    "\n",
    "    # Fingertip Visualization\n",
    "    if dpg.get_value('source') == 'Masked Edge': pass\n",
    "    elif dpg.get_value('source') == 'Virtual FOV':\n",
    "        try:\n",
    "            # virtual_fov = visualize_tips(tip_pos, tip_dir, virtual_fov) # finger marker only\n",
    "            virtual_fov = visualize_tiplets3(*tip_info(mp_results, img), virtual_fov) # tiplets only (best)\n",
    "            # virtual_fov = visualize_tiplets(*tip_info(mp_results, img), visualize_tips(*tip_info(mp_results, img), virtual_fov)) # both (eh)\n",
    "        except Exception: pass # no hand detected\n",
    "\n",
    "    \"\"\"Object Tracking\"\"\" # TODO wrap into a function\n",
    "    # boxes, scores, class_ids = yolov7_detector(img)\n",
    "    # dets = np.concatenate((boxes, scores[:,np.newaxis]),axis=1)\n",
    "\n",
    "    # # update SORT\n",
    "    # track_bbs_ids = mot_tracker.update(dets)\n",
    "    # track_bbs_ids.astype(np.int32)\n",
    "\n",
    "    # draw path visualizations on OFA output window (leave original and overwrite on top)\n",
    "    \n",
    "    # 음성인식 추가 (via Colab)\n",
    "\n",
    "    # TipLets, WristLets output computation.\n",
    "\n",
    "\n",
    "    \"\"\"Push Textures\"\"\"\n",
    "    dpg.set_value(\"original\",      imdpg2(img))\n",
    "    dpg.set_value(\"edge\",          imdpg2(edge)) \n",
    "    dpg.set_value(\"segmentation\",  imdpg2(seg_overlay))\n",
    "    dpg.set_value(\"masked_edge\",   imdpg2(masked_edge)) # NOTE can either use rgb & blend, or rgba & no blend.\n",
    "    dpg.set_value(\"virtual_fov\",   imdpg2(virtual_fov))\n",
    "    dpg.set_value(\"hand_tracking\", imdpg2(hand_tracking))\n",
    "    dpg.set_value(\"tiplets\",       imdpg2(tiplets))\n",
    "    dpg.set_value(\"ofa\",           imdpg2(ofa))\n",
    "\n",
    "\n",
    "    \"\"\"Push Frame\"\"\"\n",
    "    dpg.render_dearpygui_frame()\n",
    "    c += 1\n",
    "\n",
    "end_time = time.time()\n",
    "time_delta = end_time - start_time\n",
    "dpg.destroy_context()\n",
    "\n",
    "print(\"Frames Per Second:\", round(c/time_delta, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "- unfortunately, the object described by user was not detected by the object detector\n",
    "  - this may be because it is an object that the model doesn't track at all,\n",
    "  - or because of model's accuracy problems, because of timing/syncing problems, or because the tracking algorithm lost track of the object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **E. Optimization** (cProfile, SnakeViz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Optimization Targets:**\n",
    "1. `pidinet, torch.cpu()`\n",
    "  - 실제 PyTorch call에 40%가량, cpu()에 60%가량 소요. 이게 의미하는 바는 아직 GPU에서 비동기적으로 돌아가는중 synchronization을 호출해서 아무것도 안 하는 상태로 기다리는 상태. 이렇게 20%가량 소요. 나머지 15%는 PiDiNet 자체의 PyTorch 호출스택이 차지하는데, TensorRT 컴파일을 마치면 아마 비동기 호출되어 아낄 수 있다 생각됨.\n",
    "    - → trivial \n",
    "2. `visualize_tiplets()`\n",
    "  - blend_two() 혼자 너무 오래 걸림. NumPy, Python 슬라이싱/수학 연산자만 있는데 전체 런타임의 24% 가량 소모. 효율적 구현 찾아서 대체하자. \n",
    "    - → trivial\n",
    "3. `SciPy dilate_mask()`\n",
    "  - SciPy의 구현이 느림. NumPy로 빠른 binary dilation 구현을 찾아 대체하자. 근데 이거 그냥 convolution 아닌가. 왜 12%씩 먹지. 또는 해상도를 낮춘 후 dilation 후 다시 늘리는 방법 또한 존재\n",
    "    - → trivial\n",
    "4. `MediaPipe process()`\n",
    "  - 비동기 호출 또는 GPU가속이 가능한지 찾아보자. \n",
    "5. `imdpg()`\n",
    "  - NumPy는 벡터연산이라 batching 하면 효과적일줄 알았지만 오히려 느림. \n",
    "    - _\n",
    "6. Yolact\n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimization Log**\n",
    "\n",
    "1. `blend_two()` -> `blend_two3`. Optimized using `cv2.addWeighted()` instead of NumPy / Python operations. FPS from 11.8 to 17.2.\n",
    "\n",
    "New Optimization Targets\n",
    "\n",
    "(image: optimization_targets_itr2.png)\n",
    "\n",
    "2. `scipy.ndimage.morphology.binary_dilation` → `grey_dilation` using **size parameter.** Using footprint, structure attributes yields similar time w/ binary_dilation. However, this isn't reproduced in benchmark_optimization notebook. **Hence, further optimization (w/o edge quality degradation) is possible.** [Related Discussion](https://github.com/scipy/scipy/issues/13991)\n",
    "\n",
    "New Optimization Targets\n",
    "\n",
    "(image: optimization_targets_itr3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed14edbca5228d46ee727e3895a84b669fc2394a68d3f600b4cef4b26033d3d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
